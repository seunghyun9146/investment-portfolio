{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ C:\\pthon_basic\\webcrolingProject\\news_crawler_project\\data\\naver_news.csv 파일이 업데이트되었습니다! (50개 뉴스 저장)\n",
      "\n",
      "📢 최신 뉴스 미리보기:\n",
      "1. 증시 활황·민생추경에 소비심리↑…“美관세에 하방압력 여전” (https://n.news.naver.com/mnews/article/018/0006068500)\n",
      "2. 롯데웰푸드, 서울 마포서 마라톤 대회 ‘2025 설레임런’ 개최 (https://n.news.naver.com/mnews/article/015/0005159634)\n",
      "3. ‘잃어버린 9년’ 무거운 짐 벗고 뉴삼성 속도낸다 (https://n.news.naver.com/mnews/article/022/0004052608)\n",
      "4. 관세 불안 잠재운 '소매 판매'…S&P·나스닥 최고치 [뉴욕증시 브리핑] (https://n.news.naver.com/mnews/article/015/0005159522)\n",
      "5. SGI서울보증, 보증서 발급 재개…주요 전산시스템 복구 (https://n.news.naver.com/mnews/article/629/0000408165)\n",
      "6. 무신사 스탠다드, 서울 동부 첫 매장 '더리버몰 강동점' 개점 (https://n.news.naver.com/mnews/article/008/0005223540)\n",
      "7. GS칼텍스, KAIST와 '다문화 과학인재양성 캠프' 개최 (https://n.news.naver.com/mnews/article/001/0015514915)\n",
      "8. \"차 고쳐달라고 맡겼더니 손상\"‥소비자원 신청만 953건 (https://n.news.naver.com/mnews/article/214/0001437144)\n",
      "9. 中-日 관광 급증에, 인천공항이 수혜..역대 최다 경신 (https://n.news.naver.com/mnews/article/016/0002501458)\n",
      "10. 더스프링시니어, TV광고 캠페인 온에어 (https://n.news.naver.com/mnews/article/081/0003558805)\n",
      "11. 80% 지었는데 공사중단 위기…잠실르엘 조합장 해임 여부 '촉각' (https://n.news.naver.com/mnews/article/629/0000408580)\n",
      "12. '관세·실적·중대법' 삼중고 포스코…이희근 대표 리더십 흔들 (https://n.news.naver.com/mnews/article/629/0000408578)\n",
      "13. 메리츠 M&A 통큰 행보…SK이노 ‘5조 유동화’ 낙점 (https://n.news.naver.com/mnews/article/016/0002501661)\n",
      "14. 다가오는 트럼프 관세…'무풍지대' 종목은? (https://n.news.naver.com/mnews/article/003/0013370934)\n",
      "15. 시총 19.3조 증발 하이닉스…“구조적 위기 아니다” (https://n.news.naver.com/mnews/article/016/0002501654)\n",
      "16. 마곡 다시 상승세…엠밸리 7단지 20억 육박 (https://n.news.naver.com/mnews/article/016/0002501653)\n",
      "17. 영등포 문래동 국화아파트 42층 단지로 재탄생[서울25] (https://n.news.naver.com/mnews/article/032/0003383694)\n",
      "18. '증권 논란’ 사라지나…가상자산 업계 ‘코인3법’ 통과 환영 (https://n.news.naver.com/mnews/article/018/0006068551)\n",
      "19. 보건의료노조, 24일 총파업…전공의 복귀 무드에도 의료사태 '안갯속' (https://n.news.naver.com/mnews/article/008/0005223588)\n",
      "20. 신라면세점 3년여간 1600억 영구채에도…부채비율 '밑 빠진 독' [넘버스] (https://n.news.naver.com/mnews/article/293/0000069973)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import schedule\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def crawl_and_save():\n",
    "    \"\"\"네이버 경제 뉴스 크롤링 후 CSV로 저장\"\"\"\n",
    "    url = \"https://news.naver.com/section/101\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # HTTP 에러 확인\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ 요청 오류 발생: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # ✅ 최대 70개까지 가져오고, 실제 저장은 50개만\n",
    "    news_list = soup.select(\"div.sa_text a\")[:70]  \n",
    "\n",
    "    def get_article_content(url):\n",
    "        \"\"\"기사 본문 가져오기\"\"\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException:\n",
    "            return \"❌ 본문을 가져올 수 없습니다.\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        possible_selectors = [\"div#newsct_article\", \"div#articleBodyContents\", \"div#dic_area\"]\n",
    "        \n",
    "        for selector in possible_selectors:\n",
    "            article = soup.select_one(selector)\n",
    "            if article:\n",
    "                return article.get_text(strip=True)\n",
    "        \n",
    "        return \"❌ 본문을 찾을 수 없습니다.\"\n",
    "\n",
    "    news_data = []\n",
    "    for news in news_list:\n",
    "        title = news.get_text(strip=True)\n",
    "        link = news[\"href\"]\n",
    "\n",
    "        # URL 정리\n",
    "        if not link.startswith(\"http\"):\n",
    "            article_url = \"https://news.naver.com\" + link\n",
    "        else:\n",
    "            article_url = link\n",
    "\n",
    "        # 필요 없는 뉴스 필터링\n",
    "        if \"더보기\" in title or len(title) < 5:\n",
    "            continue\n",
    "        \n",
    "        article_content = get_article_content(article_url)\n",
    "        if \"❌ 본문\" in article_content:\n",
    "            continue\n",
    "\n",
    "        news_data.append((title, article_url, article_content))\n",
    "\n",
    "        # ✅ 뉴스 저장 개수를 50개로 변경\n",
    "        if len(news_data) >= 50:\n",
    "            break\n",
    "\n",
    "    # CSV 파일로 저장 (덮어쓰기 모드)\n",
    "    csv_filename = r\"C:\\pthon_basic\\webcrolingProject\\news_crawler_project\\data\\naver_news.csv\"\n",
    "    with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"제목\", \"링크\", \"본문\"])\n",
    "        writer.writerows(news_data)\n",
    "\n",
    "    # 화면 클리어 & 진행 상태 업데이트\n",
    "    clear_output(wait=True)\n",
    "    print(f\"✅ {csv_filename} 파일이 업데이트되었습니다! (50개 뉴스 저장)\")\n",
    "    display_news(news_data)\n",
    "\n",
    "def display_news(news_data):\n",
    "    \"\"\"크롤링된 뉴스 제목 10개만 미리보기\"\"\"\n",
    "    print(\"\\n📢 최신 뉴스 미리보기:\")\n",
    "    for idx, (title, link, _) in enumerate(news_data[:20], 1):\n",
    "        print(f\"{idx}. {title} ({link})\")\n",
    "\n",
    "# ✅ 5분마다 실행하도록 설정\n",
    "schedule.every(5).minutes.do(crawl_and_save)\n",
    "\n",
    "print(\"✅ Jupyter Notebook에서 5분마다 뉴스 크롤링이 실행됩니다.\")\n",
    "\n",
    "# **5분마다 실행되는 루프 (Jupyter 실행 중지하면 멈춤)**\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)  # 1분마다 실행 여부 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ naver_news.csv 파일이 업데이트되었습니다! (100개 뉴스 저장)\n",
      "⏳ 실행 완료! 걸린 시간: 7.54초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# ✅ 네이버 경제 뉴스 URL\n",
    "URL = \"https://news.naver.com/section/101\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def get_article_content(url):\n",
    "    \"\"\"📌 네이버 뉴스 기사 본문 가져오기\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"❌ 본문을 가져올 수 없습니다.\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    possible_selectors = [\"div#newsct_article\", \"div#articleBodyContents\", \"div#dic_area\"]\n",
    "\n",
    "    for selector in possible_selectors:\n",
    "        article = soup.select_one(selector)\n",
    "        if article:\n",
    "            return article.get_text(strip=True)\n",
    "    \n",
    "    return \"❌ 본문을 찾을 수 없습니다.\"\n",
    "\n",
    "def crawl_naver_news():\n",
    "    \"\"\"📌 네이버 경제 뉴스 크롤링 (100개)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(URL, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ 요청 오류 발생: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    news_list = soup.select(\"div.sa_text a\")[:150]  # ✅ 최대 150개 크롤링 후 필터링\n",
    "\n",
    "    news_data = []\n",
    "    for news in news_list:\n",
    "        title = news.get_text(strip=True)\n",
    "        link = news[\"href\"]\n",
    "\n",
    "        # URL 정리\n",
    "        if not link.startswith(\"http\"):\n",
    "            article_url = \"https://news.naver.com\" + link\n",
    "        else:\n",
    "            article_url = link\n",
    "\n",
    "        # 필요 없는 뉴스 필터링\n",
    "        if \"더보기\" in title or len(title) < 5:\n",
    "            continue\n",
    "        \n",
    "        # ✅ 기사 본문 가져오기\n",
    "        article_content = get_article_content(article_url)\n",
    "        if \"❌ 본문\" in article_content:\n",
    "            continue\n",
    "\n",
    "        news_data.append((title, article_url, article_content))\n",
    "\n",
    "        # ✅ 뉴스 100개가 채워지면 종료\n",
    "        if len(news_data) >= 100:\n",
    "            break\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def save_news_to_csv(news_data, filename=\"naver_news.csv\"):\n",
    "    \"\"\"📌 크롤링한 뉴스 데이터를 CSV로 저장\"\"\"\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"제목\", \"링크\", \"본문\"])\n",
    "        writer.writerows(news_data)\n",
    "\n",
    "    print(f\"✅ {filename} 파일이 업데이트되었습니다! (100개 뉴스 저장)\")\n",
    "\n",
    "# ✅ 실행\n",
    "start_time = time.time()\n",
    "news_data = crawl_naver_news()\n",
    "save_news_to_csv(news_data)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"⏳ 실행 완료! 걸린 시간: {round(end_time - start_time, 2)}초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 마지막 업데이트 시간: Sat Mar  8 16:44:19 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "csv_filename = \"naver_news.csv\"\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    modified_time = os.path.getmtime(csv_filename)\n",
    "    print(\"📅 마지막 업데이트 시간:\", time.ctime(modified_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ C:/pthon_basic/webcrolingProject/news_crawler_project/data/naver_news.csv 파일이 업데이트되었습니다! (100개 뉴스 저장)\n",
      "⏳ 실행 완료! 걸린 시간: 8.95초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# ✅ 네이버 경제 뉴스 URL\n",
    "URL = \"https://news.naver.com/section/101\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def get_article_content_and_date(url):\n",
    "    \"\"\"📌 네이버 뉴스 기사 본문 + 날짜 가져오기\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"❌ 본문을 가져올 수 없습니다.\", \"❌ 날짜 없음\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    possible_selectors = [\"div#newsct_article\", \"div#articleBodyContents\", \"div#dic_area\"]\n",
    "    date_selector = [\"span#newsct_article_date\", \"span.media_end_head_info_datestamp_time\", \"span.article_date\"]\n",
    "\n",
    "    # ✅ 기사 본문 가져오기\n",
    "    article_content = \"❌ 본문을 찾을 수 없습니다.\"\n",
    "    for selector in possible_selectors:\n",
    "        article = soup.select_one(selector)\n",
    "        if article:\n",
    "            article_content = article.get_text(strip=True)\n",
    "            break\n",
    "    \n",
    "    # ✅ 날짜 가져오기\n",
    "    article_date = \"❌ 날짜 없음\"\n",
    "    for selector in date_selector:\n",
    "        date_tag = soup.select_one(selector)\n",
    "        if date_tag:\n",
    "            article_date = date_tag.get_text(strip=True).split()[0]  # 'YYYY-MM-DD' 형태로 저장\n",
    "            break\n",
    "\n",
    "    return article_content, article_date\n",
    "\n",
    "def crawl_naver_news():\n",
    "    \"\"\"📌 네이버 경제 뉴스 크롤링 (100개)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(URL, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ 요청 오류 발생: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    news_list = soup.select(\"div.sa_text a\")[:150]  # ✅ 최대 150개 크롤링 후 필터링\n",
    "\n",
    "    news_data = []\n",
    "    for news in news_list:\n",
    "        title = news.get_text(strip=True)\n",
    "        link = news[\"href\"]\n",
    "\n",
    "        # URL 정리\n",
    "        if not link.startswith(\"http\"):\n",
    "            article_url = \"https://news.naver.com\" + link\n",
    "        else:\n",
    "            article_url = link\n",
    "\n",
    "        # 필요 없는 뉴스 필터링\n",
    "        if \"더보기\" in title or len(title) < 5:\n",
    "            continue\n",
    "        \n",
    "        # ✅ 기사 본문 + 날짜 가져오기\n",
    "        article_content, article_date = get_article_content_and_date(article_url)\n",
    "        if \"❌ 본문\" in article_content or \"❌ 날짜\" in article_date:\n",
    "            continue\n",
    "\n",
    "        news_data.append((title, article_url, article_content, article_date))\n",
    "\n",
    "        # ✅ 뉴스 100개가 채워지면 종료\n",
    "        if len(news_data) >= 100:\n",
    "            break\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def save_news_to_csv(news_data, filename=\"C:/pthon_basic/webcrolingProject/news_crawler_project/data/naver_news.csv\"):\n",
    "    \"\"\"📌 크롤링한 뉴스 데이터를 CSV로 저장\"\"\"\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"제목\", \"링크\", \"본문\", \"날짜\"])\n",
    "        writer.writerows(news_data)\n",
    "\n",
    "    print(f\"✅ {filename} 파일이 업데이트되었습니다! (100개 뉴스 저장)\")\n",
    "\n",
    "\n",
    "# ✅ 실행\n",
    "start_time = time.time()\n",
    "news_data = crawl_naver_news()\n",
    "save_news_to_csv(news_data)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"⏳ 실행 완료! 걸린 시간: {round(end_time - start_time, 2)}초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
