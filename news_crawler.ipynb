{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… C:\\pthon_basic\\webcrolingProject\\news_crawler_project\\data\\naver_news.csv íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤! (50ê°œ ë‰´ìŠ¤ ì €ì¥)\n",
      "\n",
      "ğŸ“¢ ìµœì‹  ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°:\n",
      "1. ì¦ì‹œ í™œí™©Â·ë¯¼ìƒì¶”ê²½ì— ì†Œë¹„ì‹¬ë¦¬â†‘â€¦â€œç¾ê´€ì„¸ì— í•˜ë°©ì••ë ¥ ì—¬ì „â€ (https://n.news.naver.com/mnews/article/018/0006068500)\n",
      "2. ë¡¯ë°ì›°í‘¸ë“œ, ì„œìš¸ ë§ˆí¬ì„œ ë§ˆë¼í†¤ ëŒ€íšŒ â€˜2025 ì„¤ë ˆì„ëŸ°â€™ ê°œìµœ (https://n.news.naver.com/mnews/article/015/0005159634)\n",
      "3. â€˜ìƒì–´ë²„ë¦° 9ë…„â€™ ë¬´ê±°ìš´ ì§ ë²—ê³  ë‰´ì‚¼ì„± ì†ë„ë‚¸ë‹¤ (https://n.news.naver.com/mnews/article/022/0004052608)\n",
      "4. ê´€ì„¸ ë¶ˆì•ˆ ì ì¬ìš´ 'ì†Œë§¤ íŒë§¤'â€¦S&PÂ·ë‚˜ìŠ¤ë‹¥ ìµœê³ ì¹˜ [ë‰´ìš•ì¦ì‹œ ë¸Œë¦¬í•‘] (https://n.news.naver.com/mnews/article/015/0005159522)\n",
      "5. SGIì„œìš¸ë³´ì¦, ë³´ì¦ì„œ ë°œê¸‰ ì¬ê°œâ€¦ì£¼ìš” ì „ì‚°ì‹œìŠ¤í…œ ë³µêµ¬ (https://n.news.naver.com/mnews/article/629/0000408165)\n",
      "6. ë¬´ì‹ ì‚¬ ìŠ¤íƒ ë‹¤ë“œ, ì„œìš¸ ë™ë¶€ ì²« ë§¤ì¥ 'ë”ë¦¬ë²„ëª° ê°•ë™ì ' ê°œì  (https://n.news.naver.com/mnews/article/008/0005223540)\n",
      "7. GSì¹¼í…ìŠ¤, KAISTì™€ 'ë‹¤ë¬¸í™” ê³¼í•™ì¸ì¬ì–‘ì„± ìº í”„' ê°œìµœ (https://n.news.naver.com/mnews/article/001/0015514915)\n",
      "8. \"ì°¨ ê³ ì³ë‹¬ë¼ê³  ë§¡ê²¼ë”ë‹ˆ ì†ìƒ\"â€¥ì†Œë¹„ìì› ì‹ ì²­ë§Œ 953ê±´ (https://n.news.naver.com/mnews/article/214/0001437144)\n",
      "9. ä¸­-æ—¥ ê´€ê´‘ ê¸‰ì¦ì—, ì¸ì²œê³µí•­ì´ ìˆ˜í˜œ..ì—­ëŒ€ ìµœë‹¤ ê²½ì‹  (https://n.news.naver.com/mnews/article/016/0002501458)\n",
      "10. ë”ìŠ¤í”„ë§ì‹œë‹ˆì–´, TVê´‘ê³  ìº í˜ì¸ ì˜¨ì—ì–´ (https://n.news.naver.com/mnews/article/081/0003558805)\n",
      "11. 80% ì§€ì—ˆëŠ”ë° ê³µì‚¬ì¤‘ë‹¨ ìœ„ê¸°â€¦ì ì‹¤ë¥´ì—˜ ì¡°í•©ì¥ í•´ì„ ì—¬ë¶€ 'ì´‰ê°' (https://n.news.naver.com/mnews/article/629/0000408580)\n",
      "12. 'ê´€ì„¸Â·ì‹¤ì Â·ì¤‘ëŒ€ë²•' ì‚¼ì¤‘ê³  í¬ìŠ¤ì½”â€¦ì´í¬ê·¼ ëŒ€í‘œ ë¦¬ë”ì‹­ í”ë“¤ (https://n.news.naver.com/mnews/article/629/0000408578)\n",
      "13. ë©”ë¦¬ì¸  M&A í†µí° í–‰ë³´â€¦SKì´ë…¸ â€˜5ì¡° ìœ ë™í™”â€™ ë‚™ì  (https://n.news.naver.com/mnews/article/016/0002501661)\n",
      "14. ë‹¤ê°€ì˜¤ëŠ” íŠ¸ëŸ¼í”„ ê´€ì„¸â€¦'ë¬´í’ì§€ëŒ€' ì¢…ëª©ì€? (https://n.news.naver.com/mnews/article/003/0013370934)\n",
      "15. ì‹œì´ 19.3ì¡° ì¦ë°œ í•˜ì´ë‹‰ìŠ¤â€¦â€œêµ¬ì¡°ì  ìœ„ê¸° ì•„ë‹ˆë‹¤â€ (https://n.news.naver.com/mnews/article/016/0002501654)\n",
      "16. ë§ˆê³¡ ë‹¤ì‹œ ìƒìŠ¹ì„¸â€¦ì— ë°¸ë¦¬ 7ë‹¨ì§€ 20ì–µ ìœ¡ë°• (https://n.news.naver.com/mnews/article/016/0002501653)\n",
      "17. ì˜ë“±í¬ ë¬¸ë˜ë™ êµ­í™”ì•„íŒŒíŠ¸ 42ì¸µ ë‹¨ì§€ë¡œ ì¬íƒ„ìƒ[ì„œìš¸25] (https://n.news.naver.com/mnews/article/032/0003383694)\n",
      "18. 'ì¦ê¶Œ ë…¼ë€â€™ ì‚¬ë¼ì§€ë‚˜â€¦ê°€ìƒìì‚° ì—…ê³„ â€˜ì½”ì¸3ë²•â€™ í†µê³¼ í™˜ì˜ (https://n.news.naver.com/mnews/article/018/0006068551)\n",
      "19. ë³´ê±´ì˜ë£Œë…¸ì¡°, 24ì¼ ì´íŒŒì—…â€¦ì „ê³µì˜ ë³µê·€ ë¬´ë“œì—ë„ ì˜ë£Œì‚¬íƒœ 'ì•ˆê°¯ì†' (https://n.news.naver.com/mnews/article/008/0005223588)\n",
      "20. ì‹ ë¼ë©´ì„¸ì  3ë…„ì—¬ê°„ 1600ì–µ ì˜êµ¬ì±„ì—ë„â€¦ë¶€ì±„ë¹„ìœ¨ 'ë°‘ ë¹ ì§„ ë…' [ë„˜ë²„ìŠ¤] (https://n.news.naver.com/mnews/article/293/0000069973)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import schedule\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def crawl_and_save():\n",
    "    \"\"\"ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ í›„ CSVë¡œ ì €ì¥\"\"\"\n",
    "    url = \"https://news.naver.com/section/101\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # HTTP ì—ëŸ¬ í™•ì¸\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # âœ… ìµœëŒ€ 70ê°œê¹Œì§€ ê°€ì ¸ì˜¤ê³ , ì‹¤ì œ ì €ì¥ì€ 50ê°œë§Œ\n",
    "    news_list = soup.select(\"div.sa_text a\")[:70]  \n",
    "\n",
    "    def get_article_content(url):\n",
    "        \"\"\"ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException:\n",
    "            return \"âŒ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        possible_selectors = [\"div#newsct_article\", \"div#articleBodyContents\", \"div#dic_area\"]\n",
    "        \n",
    "        for selector in possible_selectors:\n",
    "            article = soup.select_one(selector)\n",
    "            if article:\n",
    "                return article.get_text(strip=True)\n",
    "        \n",
    "        return \"âŒ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    news_data = []\n",
    "    for news in news_list:\n",
    "        title = news.get_text(strip=True)\n",
    "        link = news[\"href\"]\n",
    "\n",
    "        # URL ì •ë¦¬\n",
    "        if not link.startswith(\"http\"):\n",
    "            article_url = \"https://news.naver.com\" + link\n",
    "        else:\n",
    "            article_url = link\n",
    "\n",
    "        # í•„ìš” ì—†ëŠ” ë‰´ìŠ¤ í•„í„°ë§\n",
    "        if \"ë”ë³´ê¸°\" in title or len(title) < 5:\n",
    "            continue\n",
    "        \n",
    "        article_content = get_article_content(article_url)\n",
    "        if \"âŒ ë³¸ë¬¸\" in article_content:\n",
    "            continue\n",
    "\n",
    "        news_data.append((title, article_url, article_content))\n",
    "\n",
    "        # âœ… ë‰´ìŠ¤ ì €ì¥ ê°œìˆ˜ë¥¼ 50ê°œë¡œ ë³€ê²½\n",
    "        if len(news_data) >= 50:\n",
    "            break\n",
    "\n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥ (ë®ì–´ì“°ê¸° ëª¨ë“œ)\n",
    "    csv_filename = r\"C:\\pthon_basic\\webcrolingProject\\news_crawler_project\\data\\naver_news.csv\"\n",
    "    with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ì œëª©\", \"ë§í¬\", \"ë³¸ë¬¸\"])\n",
    "        writer.writerows(news_data)\n",
    "\n",
    "    # í™”ë©´ í´ë¦¬ì–´ & ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "    clear_output(wait=True)\n",
    "    print(f\"âœ… {csv_filename} íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤! (50ê°œ ë‰´ìŠ¤ ì €ì¥)\")\n",
    "    display_news(news_data)\n",
    "\n",
    "def display_news(news_data):\n",
    "    \"\"\"í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ì œëª© 10ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "    print(\"\\nğŸ“¢ ìµœì‹  ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    for idx, (title, link, _) in enumerate(news_data[:20], 1):\n",
    "        print(f\"{idx}. {title} ({link})\")\n",
    "\n",
    "# âœ… 5ë¶„ë§ˆë‹¤ ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •\n",
    "schedule.every(5).minutes.do(crawl_and_save)\n",
    "\n",
    "print(\"âœ… Jupyter Notebookì—ì„œ 5ë¶„ë§ˆë‹¤ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n",
    "\n",
    "# **5ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ë£¨í”„ (Jupyter ì‹¤í–‰ ì¤‘ì§€í•˜ë©´ ë©ˆì¶¤)**\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… naver_news.csv íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤! (100ê°œ ë‰´ìŠ¤ ì €ì¥)\n",
      "â³ ì‹¤í–‰ ì™„ë£Œ! ê±¸ë¦° ì‹œê°„: 7.54ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ URL\n",
    "URL = \"https://news.naver.com/section/101\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def get_article_content(url):\n",
    "    \"\"\"ğŸ“Œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"âŒ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    possible_selectors = [\"div#newsct_article\", \"div#articleBodyContents\", \"div#dic_area\"]\n",
    "\n",
    "    for selector in possible_selectors:\n",
    "        article = soup.select_one(selector)\n",
    "        if article:\n",
    "            return article.get_text(strip=True)\n",
    "    \n",
    "    return \"âŒ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "def crawl_naver_news():\n",
    "    \"\"\"ğŸ“Œ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ (100ê°œ)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(URL, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    news_list = soup.select(\"div.sa_text a\")[:150]  # âœ… ìµœëŒ€ 150ê°œ í¬ë¡¤ë§ í›„ í•„í„°ë§\n",
    "\n",
    "    news_data = []\n",
    "    for news in news_list:\n",
    "        title = news.get_text(strip=True)\n",
    "        link = news[\"href\"]\n",
    "\n",
    "        # URL ì •ë¦¬\n",
    "        if not link.startswith(\"http\"):\n",
    "            article_url = \"https://news.naver.com\" + link\n",
    "        else:\n",
    "            article_url = link\n",
    "\n",
    "        # í•„ìš” ì—†ëŠ” ë‰´ìŠ¤ í•„í„°ë§\n",
    "        if \"ë”ë³´ê¸°\" in title or len(title) < 5:\n",
    "            continue\n",
    "        \n",
    "        # âœ… ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        article_content = get_article_content(article_url)\n",
    "        if \"âŒ ë³¸ë¬¸\" in article_content:\n",
    "            continue\n",
    "\n",
    "        news_data.append((title, article_url, article_content))\n",
    "\n",
    "        # âœ… ë‰´ìŠ¤ 100ê°œê°€ ì±„ì›Œì§€ë©´ ì¢…ë£Œ\n",
    "        if len(news_data) >= 100:\n",
    "            break\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def save_news_to_csv(news_data, filename=\"naver_news.csv\"):\n",
    "    \"\"\"ğŸ“Œ í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥\"\"\"\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ì œëª©\", \"ë§í¬\", \"ë³¸ë¬¸\"])\n",
    "        writer.writerows(news_data)\n",
    "\n",
    "    print(f\"âœ… {filename} íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤! (100ê°œ ë‰´ìŠ¤ ì €ì¥)\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "start_time = time.time()\n",
    "news_data = crawl_naver_news()\n",
    "save_news_to_csv(news_data)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"â³ ì‹¤í–‰ ì™„ë£Œ! ê±¸ë¦° ì‹œê°„: {round(end_time - start_time, 2)}ì´ˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„: Sat Mar  8 16:44:19 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "csv_filename = \"naver_news.csv\"\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    modified_time = os.path.getmtime(csv_filename)\n",
    "    print(\"ğŸ“… ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„:\", time.ctime(modified_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… C:/pthon_basic/webcrolingProject/news_crawler_project/data/naver_news.csv íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤! (100ê°œ ë‰´ìŠ¤ ì €ì¥)\n",
      "â³ ì‹¤í–‰ ì™„ë£Œ! ê±¸ë¦° ì‹œê°„: 8.95ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ URL\n",
    "URL = \"https://news.naver.com/section/101\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def get_article_content_and_date(url):\n",
    "    \"\"\"ğŸ“Œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ + ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"âŒ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\", \"âŒ ë‚ ì§œ ì—†ìŒ\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    possible_selectors = [\"div#newsct_article\", \"div#articleBodyContents\", \"div#dic_area\"]\n",
    "    date_selector = [\"span#newsct_article_date\", \"span.media_end_head_info_datestamp_time\", \"span.article_date\"]\n",
    "\n",
    "    # âœ… ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    article_content = \"âŒ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    for selector in possible_selectors:\n",
    "        article = soup.select_one(selector)\n",
    "        if article:\n",
    "            article_content = article.get_text(strip=True)\n",
    "            break\n",
    "    \n",
    "    # âœ… ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°\n",
    "    article_date = \"âŒ ë‚ ì§œ ì—†ìŒ\"\n",
    "    for selector in date_selector:\n",
    "        date_tag = soup.select_one(selector)\n",
    "        if date_tag:\n",
    "            article_date = date_tag.get_text(strip=True).split()[0]  # 'YYYY-MM-DD' í˜•íƒœë¡œ ì €ì¥\n",
    "            break\n",
    "\n",
    "    return article_content, article_date\n",
    "\n",
    "def crawl_naver_news():\n",
    "    \"\"\"ğŸ“Œ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ (100ê°œ)\"\"\"\n",
    "    try:\n",
    "        response = requests.get(URL, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    news_list = soup.select(\"div.sa_text a\")[:150]  # âœ… ìµœëŒ€ 150ê°œ í¬ë¡¤ë§ í›„ í•„í„°ë§\n",
    "\n",
    "    news_data = []\n",
    "    for news in news_list:\n",
    "        title = news.get_text(strip=True)\n",
    "        link = news[\"href\"]\n",
    "\n",
    "        # URL ì •ë¦¬\n",
    "        if not link.startswith(\"http\"):\n",
    "            article_url = \"https://news.naver.com\" + link\n",
    "        else:\n",
    "            article_url = link\n",
    "\n",
    "        # í•„ìš” ì—†ëŠ” ë‰´ìŠ¤ í•„í„°ë§\n",
    "        if \"ë”ë³´ê¸°\" in title or len(title) < 5:\n",
    "            continue\n",
    "        \n",
    "        # âœ… ê¸°ì‚¬ ë³¸ë¬¸ + ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°\n",
    "        article_content, article_date = get_article_content_and_date(article_url)\n",
    "        if \"âŒ ë³¸ë¬¸\" in article_content or \"âŒ ë‚ ì§œ\" in article_date:\n",
    "            continue\n",
    "\n",
    "        news_data.append((title, article_url, article_content, article_date))\n",
    "\n",
    "        # âœ… ë‰´ìŠ¤ 100ê°œê°€ ì±„ì›Œì§€ë©´ ì¢…ë£Œ\n",
    "        if len(news_data) >= 100:\n",
    "            break\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def save_news_to_csv(news_data, filename=\"C:/pthon_basic/webcrolingProject/news_crawler_project/data/naver_news.csv\"):\n",
    "    \"\"\"ğŸ“Œ í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥\"\"\"\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ì œëª©\", \"ë§í¬\", \"ë³¸ë¬¸\", \"ë‚ ì§œ\"])\n",
    "        writer.writerows(news_data)\n",
    "\n",
    "    print(f\"âœ… {filename} íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤! (100ê°œ ë‰´ìŠ¤ ì €ì¥)\")\n",
    "\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "start_time = time.time()\n",
    "news_data = crawl_naver_news()\n",
    "save_news_to_csv(news_data)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"â³ ì‹¤í–‰ ì™„ë£Œ! ê±¸ë¦° ì‹œê°„: {round(end_time - start_time, 2)}ì´ˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
